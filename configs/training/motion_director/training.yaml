image_finetune: false
output_dir: "outputs"

pretrained_model_path: ""
motion_module_path: ""
domain_adapter_path: ""

unet_additional_kwargs:
  use_inflated_groupnorm:     true
  use_motion_module:          true
  motion_module_resolutions:  [1,2,4,8]
  motion_module_mid_block:    false
  motion_module_type:         Vanilla

  motion_module_kwargs:
    num_attention_heads:                 8
    num_transformer_block:               1
    attention_block_types:               [ "Temporal_Self", "Temporal_Self" ]
    temporal_position_encoding:          true
    temporal_position_encoding_max_len:  32
    temporal_attention_dim_div:          1
    zero_initialize:                     true

noise_scheduler_kwargs:
  num_train_timesteps: 1000
  beta_start:    0.00085
  beta_end:      0.012
  beta_schedule: "linear"
  clip_sample:   False

use_text_augmenter: False
dataset_types: ["single_video"]

cfg_random_null_text_ratio: 0

train_data:
  manual_sample_size: False
  sample_size: [512, 512] # [height, width]
  # The width and height in which you want your training data to be resized to.
  width: 512
  height: 512

  # This will find the closest aspect ratio to your input width and height. 
  # For example, 512x512 width and height with a video of resolution 1280x720 will be resized to 512x256
  use_bucketing: False

  # The start frame index where your videos should start (Leave this at one for json and folder based training).
  sample_start_idx: 0

  # Used for 'folder'. The rate at which your frames are sampled.
  fps: 0

  # For 'single_video' and 'json'. The number of frames to "step" (1,2,3,4) (frame_step=2) -> (1,3,5,7, ...).  
  frame_step: 3

  # The number of frames to sample. The higher this number, the higher the VRAM (acts similar to batch size).
  n_sample_frames: 16

  # For validation
  sample_n_frames: 16
  
  # 'single_video'
  single_video_path: ""

  # The prompt when using a single video file
  single_video_prompt: ""

  fallback_prompt: ""

  max_chunks: 1

validation_data:
  prompts:
    - ""
  num_inference_steps: 25
  guidance_scale: 9
  spatial_scale: 0.5
  validation_seed: 44

lora_name: ""
use_motion_lora_format: True
lora_rank: 32
lora_unet_dropout: 0.1
single_spatial_lora: True
train_sample_validation: False
unet_checkpoint_path: ""

learning_rate:    5e-4
learning_rate_spatial:    1e-4
adam_weight_decay: 1e-2
cache_latents: true
train_batch_size: 1
use_lion_optim: True
use_offset_noise: False

max_train_epoch:      503
max_train_steps:      -1
checkpointing_epochs: -1
checkpointing_steps:  100

validation_steps:       50
validation_steps_tuple: [2, 50]

global_seed: 33
mixed_precision_training: true
enable_xformers_memory_efficient_attention: True
gradient_checkpointing: True

is_debug: False
